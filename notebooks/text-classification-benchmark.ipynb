{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/PrithivirajDamodaran/Parrot_Paraphraser.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "# Ensure NLTK WordNet is downloaded\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function to get synonyms\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name().replace('_', ' '))\n",
    "    return list(synonyms)\n",
    "\n",
    "# Function to introduce controlled scrambling\n",
    "def scramble_text(text):\n",
    "    words = text.split()\n",
    "    scrambled_text = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Randomly replace with a synonym\n",
    "        synonyms = get_synonyms(word)\n",
    "        if synonyms and random.random() < 0.3:  # 30% chance to replace\n",
    "            scrambled_text.append(random.choice(synonyms))\n",
    "        else:\n",
    "            # Occasionally add noise to the word\n",
    "            if random.random() < 0.2:  # 20% chance to add noise\n",
    "                word = \"\".join(random.sample(word, len(word)))  # Shuffle characters\n",
    "            scrambled_text.append(word)\n",
    "        \n",
    "        # Randomly insert noise words\n",
    "        if random.random() < 0.2:  # 20% chance to insert noise word\n",
    "            scrambled_text.append(random.choice([\"extra\", \"random\", \"noise\", \"confuse\"]))\n",
    "    \n",
    "    # Shuffle chunks of the text\n",
    "    chunk_size = max(1, len(scrambled_text) // 3)\n",
    "    chunks = [scrambled_text[i:i+chunk_size] for i in range(0, len(scrambled_text), chunk_size)]\n",
    "    random.shuffle(chunks)\n",
    "    shuffled_text = [word for chunk in chunks for word in chunk]\n",
    "    \n",
    "    return \" \".join(shuffled_text)\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\Suhaa\\Downloads\\Codey AI\\Mockups\\navigator-batch-generate-6768423f003f7114aba22d03-data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Apply scrambling\n",
    "data['User Command'] = data['User Command'].apply(scramble_text)\n",
    "\n",
    "# Save scrambled data\n",
    "output_path = r\"C:\\Users\\Suhaa\\Downloads\\Codey AI\\Mockups\\challenging_scrambled_data.csv\"\n",
    "data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Challenging scrambled dataset saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    ")\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load data\n",
    "file_path = r\"C:\\Users\\Suhaa\\Downloads\\Codey AI\\Mockups\\challenging_scrambled_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Initial Data Shape:\", df.shape)\n",
    "print(\"Initial Data Sample:\")\n",
    "print(df.head())\n",
    "\n",
    "df = df.dropna(subset=[\"User Command\", \"Action\"]).drop_duplicates()\n",
    "print(\"\\nData Shape after Dropping NA and Duplicates:\", df.shape)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Action_encoded\"] = label_encoder.fit_transform(df[\"Action\"])\n",
    "\n",
    "print(\"\\nEncoded Classes:\")\n",
    "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
    "\n",
    "# Create features\n",
    "X = df[\"User Command\"].values\n",
    "y = df[\"Action_encoded\"].values\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "print(\"\\nTF-IDF Feature Shape:\", X_tfidf.shape)\n",
    "\n",
    "# Balance classes\n",
    "original_class_counts = Counter(y)\n",
    "print(\"\\nOriginal Class Distribution:\", original_class_counts)\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_tfidf, y)\n",
    "\n",
    "new_class_counts = Counter(y_resampled)\n",
    "print(\"After Random Over-Sampling Class Distribution:\", new_class_counts)\n",
    "\n",
    "# Models\n",
    "nb_model = MultinomialNB()\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "\n",
    "def create_neural_net(input_dim, output_dim):\n",
    "    model = Sequential([\n",
    "        Dense(256, activation=\"relu\", input_dim=input_dim),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_dim, activation=\"softmax\")\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def evaluate_model(y_true, y_pred, y_pred_proba=None, multi_class='ovr'):\n",
    "    metrics = {}\n",
    "    metrics['Accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['Precision'] = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['Recall'] = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['F1-Score'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    if y_pred_proba is not None and multi_class is not None:\n",
    "        try:\n",
    "            y_binarized = label_binarize(y_true, classes=np.unique(y_true))\n",
    "            if y_binarized.shape[1] == 1:\n",
    "                roc_auc = roc_auc_score(y_true, y_pred_proba[:, 1])\n",
    "            else:\n",
    "                roc_auc = roc_auc_score(y_binarized, y_pred_proba, average='weighted', multi_class=multi_class)\n",
    "            metrics['ROC-AUC'] = roc_auc\n",
    "        except:\n",
    "            metrics['ROC-AUC'] = 'N/A'\n",
    "    else:\n",
    "        metrics['ROC-AUC'] = 'N/A'\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def get_data_subset(X, y, reduction_ratio):\n",
    "    if reduction_ratio == 1.0:\n",
    "        return X, y\n",
    "    else:\n",
    "        X_subset, _, y_subset, _ = train_test_split(\n",
    "            X, y,\n",
    "            train_size=reduction_ratio,\n",
    "            stratify=y,\n",
    "            random_state=42\n",
    "        )\n",
    "        return X_subset, y_subset\n",
    "\n",
    "results = []\n",
    "data_reduction_levels = [1.0, 0.75, 0.5, 0.25, 0.1]\n",
    "for reduction in data_reduction_levels:\n",
    "    print(f\"\\n---\\nEvaluating Models with {int(reduction*100)}% of Data\\n---\")\n",
    "    \n",
    "    X_subset, y_subset = get_data_subset(X_resampled, y_resampled, reduction)\n",
    "    \n",
    "    print(\"Subset Class Distribution:\", Counter(y_subset))\n",
    "    print(\"Subset Feature Shape:\", X_subset.shape)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_subset, y_subset,\n",
    "        test_size=0.2,\n",
    "        stratify=y_subset,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Naive Bayes\n",
    "    print(\"\\nTraining and Evaluating Multinomial Naive Bayes...\")\n",
    "    start_time = time.time()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    y_pred_nb = nb_model.predict(X_test)\n",
    "    y_pred_proba_nb = nb_model.predict_proba(X_test)\n",
    "    metrics_nb = evaluate_model(y_test, y_pred_nb, y_pred_proba_nb, multi_class='ovr')\n",
    "    \n",
    "    sample = X_test[0]\n",
    "    start_proc_time = time.time()\n",
    "    nb_model.predict(sample)\n",
    "    processing_time = (time.time() - start_proc_time) * 1000\n",
    "    \n",
    "    metrics_nb['Data Size (%)'] = int(reduction*100)\n",
    "    metrics_nb['Model'] = 'NLP Model (MultinomialNB)'\n",
    "    metrics_nb['Training Time (s)'] = round(training_time, 4)\n",
    "    metrics_nb['Processing Time (ms)'] = round(processing_time, 4)\n",
    "    results.append(metrics_nb)\n",
    "    \n",
    "    # Decision Tree\n",
    "    print(\"\\nTraining and Evaluating Decision Tree...\")\n",
    "    start_time = time.time()\n",
    "    dt_model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    y_pred_dt = dt_model.predict(X_test)\n",
    "    y_pred_proba_dt = dt_model.predict_proba(X_test)\n",
    "    metrics_dt = evaluate_model(y_test, y_pred_dt, y_pred_proba_dt, multi_class='ovr')\n",
    "    \n",
    "    sample = X_test[0]\n",
    "    start_proc_time = time.time()\n",
    "    dt_model.predict(sample)\n",
    "    processing_time = (time.time() - start_proc_time) * 1000\n",
    "    \n",
    "    metrics_dt['Data Size (%)'] = int(reduction*100)\n",
    "    metrics_dt['Model'] = 'Decision Tree'\n",
    "    metrics_dt['Training Time (s)'] = round(training_time, 4)\n",
    "    metrics_dt['Processing Time (ms)'] = round(processing_time, 4)\n",
    "    results.append(metrics_dt)\n",
    "    \n",
    "    # Random Forest\n",
    "    print(\"\\nTraining and Evaluating Random Forest...\")\n",
    "    start_time = time.time()\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    y_pred_proba_rf = rf_model.predict_proba(X_test)\n",
    "    metrics_rf = evaluate_model(y_test, y_pred_rf, y_pred_proba_rf, multi_class='ovr')\n",
    "    \n",
    "    sample = X_test[0]\n",
    "    start_proc_time = time.time()\n",
    "    rf_model.predict(sample)\n",
    "    processing_time = (time.time() - start_proc_time) * 1000\n",
    "    \n",
    "    metrics_rf['Data Size (%)'] = int(reduction*100)\n",
    "    metrics_rf['Model'] = 'Random Forest'\n",
    "    metrics_rf['Training Time (s)'] = round(training_time, 4)\n",
    "    metrics_rf['Processing Time (ms)'] = round(processing_time, 4)\n",
    "    results.append(metrics_rf)\n",
    "    \n",
    "    # Neural Network\n",
    "    print(\"\\nTraining and Evaluating Neural Decision Tree...\")\n",
    "    ndt_model = create_neural_net(input_dim=X_train.shape[1], output_dim=len(label_encoder.classes_))\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = ndt_model.fit(\n",
    "        X_train.toarray(), y_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test.toarray(), y_test),\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    y_pred_ndt_prob = ndt_model.predict(X_test.toarray())\n",
    "    y_pred_ndt = np.argmax(y_pred_ndt_prob, axis=1)\n",
    "    metrics_ndt = evaluate_model(y_test, y_pred_ndt, y_pred_ndt_prob, multi_class='ovr')\n",
    "    \n",
    "    sample = X_test[0].toarray()\n",
    "    start_proc_time = time.time()\n",
    "    ndt_model.predict(sample)\n",
    "    processing_time = (time.time() - start_proc_time) * 1000\n",
    "    \n",
    "    metrics_ndt['Data Size (%)'] = int(reduction*100)\n",
    "    metrics_ndt['Model'] = 'Neural Decision Tree'\n",
    "    metrics_ndt['Training Time (s)'] = round(training_time, 4)\n",
    "    metrics_ndt['Processing Time (ms)'] = round(processing_time, 4)\n",
    "    results.append(metrics_ndt)\n",
    "\n",
    "# Create final results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df[['Data Size (%)', 'Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'Training Time (s)', 'Processing Time (ms)']]\n",
    "\n",
    "print(\"\\nFinal Evaluation Results:\")\n",
    "print(results_df)\n",
    "\n",
    "# results_df.to_csv('model_evaluation_comparison.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    ")\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Different neural network architectures\n",
    "def create_ndt_config_1(input_dim, output_dim):\n",
    "    model = Sequential([\n",
    "        Dense(256, activation=\"relu\", input_dim=input_dim),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_dim, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss=\"sparse_categorical_crossentropy\", \n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def create_ndt_config_2(input_dim, output_dim):\n",
    "    model = Sequential([\n",
    "        Dense(512, activation=\"relu\", input_dim=input_dim),\n",
    "        Dropout(0.4),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_dim, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), \n",
    "                  loss=\"sparse_categorical_crossentropy\", \n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def create_ndt_config_3(input_dim, output_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation=\"relu\", input_dim=input_dim),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dense(output_dim, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss=\"sparse_categorical_crossentropy\", \n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def create_ndt_config_4(input_dim, output_dim):\n",
    "    model = Sequential([\n",
    "        Dense(300, activation=\"relu\", input_dim=input_dim),\n",
    "        Dropout(0.5),\n",
    "        Dense(200, activation=\"relu\"),\n",
    "        Dropout(0.3),\n",
    "        Dense(100, activation=\"relu\"),\n",
    "        Dense(output_dim, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "                  loss=\"sparse_categorical_crossentropy\", \n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def create_ndt_config_5(input_dim, output_dim):\n",
    "    model = Sequential([\n",
    "        Dense(256, activation=\"relu\", input_dim=input_dim),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_dim, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0015), \n",
    "                  loss=\"sparse_categorical_crossentropy\", \n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def evaluate_model(y_true, y_pred, y_pred_proba=None, multi_class='ovr'):\n",
    "    metrics = {}\n",
    "    metrics['Accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['Precision'] = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['Recall'] = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['F1-Score'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    if y_pred_proba is not None and multi_class is not None:\n",
    "        try:\n",
    "            y_binarized = label_binarize(y_true, classes=np.unique(y_true))\n",
    "            if y_binarized.shape[1] == 1:\n",
    "                roc_auc = roc_auc_score(y_true, y_pred_proba[:, 1])\n",
    "            else:\n",
    "                roc_auc = roc_auc_score(y_binarized, y_pred_proba, average='weighted', multi_class=multi_class)\n",
    "            metrics['ROC-AUC'] = roc_auc\n",
    "        except:\n",
    "            metrics['ROC-AUC'] = 'N/A'\n",
    "    else:\n",
    "        metrics['ROC-AUC'] = 'N/A'\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def test_all_models(X_train, y_train, X_test, y_test):\n",
    "    results = []\n",
    "\n",
    "    # Naive Bayes\n",
    "    print(\"\\nTesting Multinomial Naive Bayes...\")\n",
    "    nb_model = MultinomialNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    y_pred_nb = nb_model.predict(X_test)\n",
    "    y_pred_proba_nb = nb_model.predict_proba(X_test)\n",
    "    metrics_nb = evaluate_model(y_test, y_pred_nb, y_pred_proba_nb)\n",
    "    metrics_nb['Model'] = 'Multinomial Naive Bayes'\n",
    "    results.append(metrics_nb)\n",
    "\n",
    "    # Decision Tree\n",
    "    print(\"\\nTesting Decision Tree...\")\n",
    "    dt_model = DecisionTreeClassifier(random_state=42)\n",
    "    dt_model.fit(X_train, y_train)\n",
    "    y_pred_dt = dt_model.predict(X_test)\n",
    "    y_pred_proba_dt = dt_model.predict_proba(X_test)\n",
    "    metrics_dt = evaluate_model(y_test, y_pred_dt, y_pred_proba_dt)\n",
    "    metrics_dt['Model'] = 'Decision Tree'\n",
    "    results.append(metrics_dt)\n",
    "\n",
    "    # Random Forest\n",
    "    print(\"\\nTesting Random Forest...\")\n",
    "    rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    y_pred_proba_rf = rf_model.predict_proba(X_test)\n",
    "    metrics_rf = evaluate_model(y_test, y_pred_rf, y_pred_proba_rf)\n",
    "    metrics_rf['Model'] = 'Random Forest'\n",
    "    results.append(metrics_rf)\n",
    "\n",
    "    # Neural network configurations\n",
    "    configs = [\n",
    "        (\"Config 1\", create_ndt_config_1),\n",
    "        (\"Config 2\", create_ndt_config_2),\n",
    "        (\"Config 3\", create_ndt_config_3),\n",
    "        (\"Config 4\", create_ndt_config_4),\n",
    "        (\"Config 5\", create_ndt_config_5),\n",
    "    ]\n",
    "\n",
    "    for name, create_model in configs:\n",
    "        print(f\"\\nTesting Neural Decision Tree - {name}...\")\n",
    "        model = create_model(input_dim=X_train.shape[1], output_dim=len(np.unique(y_train)))\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "        model.fit(\n",
    "            X_train.toarray(), y_train,\n",
    "            epochs=20,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        y_pred_ndt_prob = model.predict(X_test.toarray())\n",
    "        y_pred_ndt = np.argmax(y_pred_ndt_prob, axis=1)\n",
    "        metrics_ndt = evaluate_model(y_test, y_pred_ndt, y_pred_ndt_prob)\n",
    "        metrics_ndt['Model'] = f'Neural Decision Tree - {name}'\n",
    "        results.append(metrics_ndt)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run tests\n",
    "file_path = r\"C:\\Users\\Suhaa\\Downloads\\Codey AI\\Mockups\\challenging_scrambled_data.csv\"\n",
    "\n",
    "results = test_all_models(X_train, y_train, X_test, y_test)\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    ")\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load original data\n",
    "file_path = r\"C:\\Users\\Suhaa\\Downloads\\Codey AI\\Mockups\\navigator-batch-generate-6768423f003f7114aba22d03-data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Initial Data Shape:\", df.shape)\n",
    "print(\"Initial Data Sample:\")\n",
    "print(df.head())\n",
    "\n",
    "df = df.dropna(subset=[\"User Command\", \"Action\"]).drop_duplicates()\n",
    "print(\"\\nData Shape after Dropping NA and Duplicates:\", df.shape)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Action_encoded\"] = label_encoder.fit_transform(df[\"Action\"])\n",
    "\n",
    "print(\"\\nEncoded Classes:\")\n",
    "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
    "\n",
    "# Prepare data\n",
    "X_raw = df[\"User Command\"].values\n",
    "y = df[\"Action_encoded\"].values\n",
    "\n",
    "# Balance classes\n",
    "original_class_counts = Counter(y)\n",
    "print(\"\\nOriginal Class Distribution:\", original_class_counts)\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled_raw, y_resampled = ros.fit_resample(X_raw.reshape(-1, 1), y)\n",
    "X_resampled_raw = X_resampled_raw.flatten()\n",
    "\n",
    "new_class_counts = Counter(y_resampled)\n",
    "print(\"After Random Over-Sampling Class Distribution:\", new_class_counts)\n",
    "\n",
    "# Create TF-IDF features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
    "X_resampled_tfidf = tfidf_vectorizer.fit_transform(X_resampled_raw)\n",
    "\n",
    "print(\"\\nTF-IDF Feature Shape:\", X_resampled_tfidf.shape)\n",
    "\n",
    "# Models\n",
    "nb_model = MultinomialNB()\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "\n",
    "def create_neural_net(input_dim, output_dim):\n",
    "    model = Sequential([\n",
    "        Dense(256, activation=\"relu\", input_dim=input_dim),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_dim, activation=\"softmax\")\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def create_bert_model(num_labels):\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "    return model\n",
    "\n",
    "def create_distilbert_model(num_labels):\n",
    "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(y_true, y_pred, y_pred_proba=None, multi_class='ovr'):\n",
    "    metrics = {}\n",
    "    metrics['Accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['Precision'] = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['Recall'] = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['F1-Score'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    if y_pred_proba is not None and multi_class is not None:\n",
    "        try:\n",
    "            y_binarized = label_binarize(y_true, classes=np.unique(y_true))\n",
    "            if y_binarized.shape[1] == 1:\n",
    "                roc_auc = roc_auc_score(y_true, y_pred_proba[:, 1])\n",
    "            else:\n",
    "                roc_auc = roc_auc_score(y_binarized, y_pred_proba, average='weighted', multi_class=multi_class)\n",
    "            metrics['ROC-AUC'] = roc_auc\n",
    "        except:\n",
    "            metrics['ROC-AUC'] = 'N/A'\n",
    "    else:\n",
    "        metrics['ROC-AUC'] = 'N/A'\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def get_data_subset(X, y, reduction_ratio):\n",
    "    if reduction_ratio == 1.0:\n",
    "        return X, y\n",
    "    else:\n",
    "        X_subset, _, y_subset, _ = train_test_split(\n",
    "            X, y,\n",
    "            train_size=reduction_ratio,\n",
    "            stratify=y,\n",
    "            random_state=42\n",
    "        )\n",
    "        return X_subset, y_subset\n",
    "\n",
    "results = []\n",
    "data_reduction_levels = [1.0, 0.75, 0.5, 0.25, 0.1]\n",
    "\n",
    "def encode_sentences_bert(sentences, labels, tokenizer, max_length=128):\n",
    "    inputs = tokenizer(list(sentences), padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], torch.tensor(labels))\n",
    "    return dataset\n",
    "\n",
    "def measure_processing_time_llm(model, tokenizer, sentence, device):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True, max_length=128).to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    processing_time_ms = (end_time - start_time) * 1000\n",
    "    return pred, processing_time_ms\n",
    "for reduction in data_reduction_levels:\n",
    "    print(f\"\\n---\\nEvaluating Models with {int(reduction*100)}% of Data\\n---\")\n",
    "    \n",
    "    X_subset_raw, y_subset = get_data_subset(X_resampled_raw, y_resampled, reduction)\n",
    "    \n",
    "    print(\"Subset Class Distribution:\", Counter(y_subset))\n",
    "    print(\"Subset Raw Text Shape:\", X_subset_raw.shape)\n",
    "    \n",
    "    X_subset_tfidf = tfidf_vectorizer.transform(X_subset_raw)\n",
    "    \n",
    "    # Split data for traditional models\n",
    "    X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(\n",
    "        X_subset_tfidf, y_subset,\n",
    "        test_size=0.2,\n",
    "        stratify=y_subset,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Split data for LLMs\n",
    "    X_train_raw, X_test_raw, y_train_llm, y_test_llm = train_test_split(\n",
    "        X_subset_raw, y_subset,\n",
    "        test_size=0.2,\n",
    "        stratify=y_subset,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Naive Bayes\n",
    "    print(\"\\nTraining and Evaluating Multinomial Naive Bayes...\")\n",
    "    start_time = time.time()\n",
    "    nb_model.fit(X_train_tfidf, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "    y_pred_proba_nb = nb_model.predict_proba(X_test_tfidf)\n",
    "    metrics_nb = evaluate_model(y_test, y_pred_nb, y_pred_proba_nb, multi_class='ovr')\n",
    "    \n",
    "    sample = X_test_tfidf[0]\n",
    "    start_proc_time = time.time()\n",
    "    nb_model.predict(sample)\n",
    "    processing_time = (time.time() - start_proc_time) * 1000\n",
    "    \n",
    "    metrics_nb['Data Size (%)'] = int(reduction*100)\n",
    "    metrics_nb['Model'] = 'NLP Model (MultinomialNB)'\n",
    "    metrics_nb['Training Time (s)'] = round(training_time, 4)\n",
    "    metrics_nb['Processing Time (ms)'] = round(processing_time, 4)\n",
    "    results.append(metrics_nb)\n",
    "    \n",
    "    # Decision Tree\n",
    "    print(\"\\nTraining and Evaluating Decision Tree...\")\n",
    "    start_time = time.time()\n",
    "    dt_model.fit(X_train_tfidf, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    y_pred_dt = dt_model.predict(X_test_tfidf)\n",
    "    y_pred_proba_dt = dt_model.predict_proba(X_test_tfidf)\n",
    "    metrics_dt = evaluate_model(y_test, y_pred_dt, y_pred_proba_dt, multi_class='ovr')\n",
    "    \n",
    "    sample = X_test_tfidf[0]\n",
    "    start_proc_time = time.time()\n",
    "    dt_model.predict(sample)\n",
    "    processing_time = (time.time() - start_proc_time) * 1000\n",
    "    \n",
    "    metrics_dt['Data Size (%)'] = int(reduction*100)\n",
    "    metrics_dt['Model'] = 'Decision Tree'\n",
    "    metrics_dt['Training Time (s)'] = round(training_time, 4)\n",
    "    metrics_dt['Processing Time (ms)'] = round(processing_time, 4)\n",
    "    results.append(metrics_dt)\n",
    "    \n",
    "    # Random Forest\n",
    "    print(\"\\nTraining and Evaluating Random Forest...\")\n",
    "    start_time = time.time()\n",
    "    rf_model.fit(X_train_tfidf, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    y_pred_rf = rf_model.predict(X_test_tfidf)\n",
    "    y_pred_proba_rf = rf_model.predict_proba(X_test_tfidf)\n",
    "    metrics_rf = evaluate_model(y_test, y_pred_rf, y_pred_proba_rf, multi_class='ovr')\n",
    "    \n",
    "    sample = X_test_tfidf[0]\n",
    "    start_proc_time = time.time()\n",
    "    rf_model.predict(sample)\n",
    "    processing_time = (time.time() - start_proc_time) * 1000\n",
    "    \n",
    "    metrics_rf['Data Size (%)'] = int(reduction*100)\n",
    "    metrics_rf['Model'] = 'Random Forest'\n",
    "    metrics_rf['Training Time (s)'] = round(training_time, 4)\n",
    "    metrics_rf['Processing Time (ms)'] = round(processing_time, 4)\n",
    "    results.append(metrics_rf)\n",
    "    \n",
    "    # Neural Network\n",
    "    print(\"\\nTraining and Evaluating Neural Decision Tree...\")\n",
    "    ndt_model = create_neural_net(input_dim=X_train_tfidf.shape[1], output_dim=len(label_encoder.classes_))\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    ndt_model.fit(\n",
    "        X_train_tfidf.toarray(), y_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test_tfidf.toarray(), y_test),\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    y_pred_ndt_prob = ndt_model.predict(X_test_tfidf.toarray())\n",
    "    y_pred_ndt = np.argmax(y_pred_ndt_prob, axis=1)\n",
    "    metrics_ndt = evaluate_model(y_test, y_pred_ndt, y_pred_ndt_prob, multi_class='ovr')\n",
    "    \n",
    "    sample = X_test_tfidf[0].toarray()\n",
    "    start_proc_time = time.time()\n",
    "    ndt_model.predict(sample)\n",
    "    processing_time = (time.time() - start_proc_time) * 1000\n",
    "    \n",
    "    metrics_ndt['Data Size (%)'] = int(reduction*100)\n",
    "    metrics_ndt['Model'] = 'Neural Decision Tree'\n",
    "    metrics_ndt['Training Time (s)'] = round(training_time, 4)\n",
    "    metrics_ndt['Processing Time (ms)'] = round(processing_time, 4)\n",
    "    results.append(metrics_ndt)\n",
    "    \n",
    "    # BERT\n",
    "    print(\"\\nTraining and Evaluating BERT (Open-Source LLM)...\")\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    bert_model.to(device)\n",
    "    \n",
    "    train_dataset_bert = encode_sentences_bert(X_train_raw, y_train, bert_tokenizer)\n",
    "    test_dataset_bert = encode_sentences_bert(X_test_raw, y_test, bert_tokenizer)\n",
    "    \n",
    "    train_loader_bert = DataLoader(train_dataset_bert, batch_size=16, shuffle=True)\n",
    "    test_loader_bert = DataLoader(test_dataset_bert, batch_size=16)\n",
    "    \n",
    "    optimizer_bert = torch.optim.AdamW(bert_model.parameters(), lr=2e-5)\n",
    "    \n",
    "    bert_model.train()\n",
    "    start_time = time.time()\n",
    "    epochs = 3\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_loader_bert:\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            optimizer_bert.zero_grad()\n",
    "            outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer_bert.step()\n",
    "        print(f\"BERT Epoch {epoch+1}/{epochs} completed.\")\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    bert_model.eval()\n",
    "    preds, true_labels, pred_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader_bert:\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds.extend(torch.argmax(probs, dim=1).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            pred_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    metrics_bert = evaluate_model(y_test, preds, pred_probs, multi_class='ovr')\n",
    "    \n",
    "    sample_sentence = X_test_raw[0]\n",
    "    pred_bert, proc_time_bert = measure_processing_time_llm(bert_model, bert_tokenizer, sample_sentence, device)\n",
    "    \n",
    "    metrics_bert['Data Size (%)'] = int(reduction*100)\n",
    "    metrics_bert['Model'] = 'BERT (Open-Source LLM)'\n",
    "    metrics_bert['Training Time (s)'] = round(training_time, 4)\n",
    "    metrics_bert['Processing Time (ms)'] = round(proc_time_bert, 4)\n",
    "    results.append(metrics_bert)\n",
    "    \n",
    "    # DistilBERT\n",
    "    print(\"\\nTraining and Evaluating DistilBERT (Lightweight Open-Source LLM)...\")\n",
    "    distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    distilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "    \n",
    "    distilbert_model.to(device)\n",
    "    \n",
    "    train_dataset_distilbert = encode_sentences_bert(X_train_raw, y_train_llm, distilbert_tokenizer)\n",
    "    test_dataset_distilbert = encode_sentences_bert(X_test_raw, y_test_llm, distilbert_tokenizer)\n",
    "    \n",
    "    train_loader_distilbert = DataLoader(train_dataset_distilbert, batch_size=16, shuffle=True)\n",
    "    test_loader_distilbert = DataLoader(test_dataset_distilbert, batch_size=16)\n",
    "    \n",
    "    optimizer_distilbert = torch.optim.AdamW(distilbert_model.parameters(), lr=2e-5)\n",
    "    \n",
    "    distilbert_model.train()\n",
    "    start_time = time.time()\n",
    "    epochs = 3\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_loader_distilbert:\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            optimizer_distilbert.zero_grad()\n",
    "            outputs = distilbert_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer_distilbert.step()\n",
    "        print(f\"DistilBERT Epoch {epoch+1}/{epochs} completed.\")\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    distilbert_model.eval()\n",
    "    preds, true_labels, pred_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader_distilbert:\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = distilbert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds.extend(torch.argmax(probs, dim=1).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            pred_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    metrics_distilbert = evaluate_model(y_test_llm, preds, pred_probs, multi_class='ovr')\n",
    "    \n",
    "    sample_sentence = X_test_raw[0]\n",
    "    pred_distilbert, proc_time_distilbert = measure_processing_time_llm(distilbert_model, distilbert_tokenizer, sample_sentence, device)\n",
    "    \n",
    "    metrics_distilbert['Data Size (%)'] = int(reduction*100)\n",
    "    metrics_distilbert['Model'] = 'DistilBERT (Lightweight LLM)'\n",
    "    metrics_distilbert['Training Time (s)'] = round(training_time, 4)\n",
    "    metrics_distilbert['Processing Time (ms)'] = round(proc_time_distilbert, 4)\n",
    "    results.append(metrics_distilbert)\n",
    "\n",
    "# Create final results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df[['Data Size (%)', 'Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'Training Time (s)', 'Processing Time (ms)']]\n",
    "\n",
    "print(\"\\nFinal Evaluation Results:\")\n",
    "print(results_df)\n",
    "\n",
    "# results_df.to_csv('model_evaluation_comparison_with_LLMs.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
